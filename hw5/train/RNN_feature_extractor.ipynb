{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "from reader import readShortVideo\n",
    "from reader import getVideoList\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import skimage.io\n",
    "import skimage\n",
    "\n",
    "import torch.nn as nn\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_valid=\"train\"):\n",
    "        if train_valid == \"train\":\n",
    "            with open(\"../train_X.pkl\", \"rb\") as f:\n",
    "                self.X = pickle.load(f)\n",
    "            with open(\"../train_y.pkl\", \"rb\") as f:\n",
    "                self.y = pickle.load(f)\n",
    "                \n",
    "        if train_valid == \"valid\":\n",
    "            with open(\"../valid_X.pkl\", \"rb\") as f:\n",
    "                self.X = pickle.load(f)\n",
    "            with open(\"../valid_y.pkl\", \"rb\") as f:\n",
    "                self.y = pickle.load(f)\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.X[index]\n",
    "        single_label = self.y[index]\n",
    "\n",
    "        # Return image and the label\n",
    "        return single_image, single_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "train_dataset = CustomDataset(\"train\")\n",
    "valid_dataset = CustomDataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_feature_extractor = torchvision.models.densenet121(pretrained=True).features.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_size = cnn_feature_extractor(train_dataset[0][0].cuda()).size()\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sequential features for RNN\n",
    "feature_size = 1024*7*7\n",
    "cnn_feature_extractor.eval()\n",
    "train_features = []\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(train_dataset.X)):\n",
    "        input_X = train_dataset[i][0]\n",
    "        feature = cnn_feature_extractor(input_X.cuda()).cpu().view(-1, feature_size)\n",
    "        train_features.append(feature)\n",
    "        counter +=1\n",
    "        if counter % 300 == 0:\n",
    "            print(counter)\n",
    "print(\"training instances done\")\n",
    "\n",
    "valid_features = []\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(valid_dataset.X)):\n",
    "        input_X = valid_dataset[i][0]\n",
    "        feature = cnn_feature_extractor(input_X.cuda()).cpu().view(-1, feature_size)\n",
    "        valid_features.append(feature)\n",
    "        counter +=1\n",
    "        if counter % 100 == 0:\n",
    "            print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../train_features_d12.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_features, f)\n",
    "with open(\"../valid_features_d12.pkl\", \"wb\") as f:\n",
    "    pickle.dump(valid_features, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training sample for seq2seq prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    '''\n",
    "    normalize for pre-trained model input\n",
    "    '''\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform_input = transforms.Compose([\n",
    "             transforms.ToPILImage(),\n",
    "             transforms.Pad((0,40), fill=0, padding_mode='constant'),\n",
    "             transforms.Resize(224),\n",
    "             # transforms.CenterCrop(224),\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    return transform_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from FullLength folder\n",
    "# training set\n",
    "print(\"training set .....\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    video_path = \"../HW5_data/FullLengthVideos/videos/train/\"\n",
    "    category_list = sorted(listdir(video_path))\n",
    "\n",
    "    category = category_list[1]\n",
    "    train_all_video_frame = []\n",
    "    cnn_feature_extractor = torchvision.models.densenet121(pretrained=True).features.cuda()\n",
    "    for category in category_list:\n",
    "        print(\"category:\",category)\n",
    "        image_list_per_folder = sorted(listdir(os.path.join(video_path,category)))\n",
    "        category_frames = []\n",
    "        for image in image_list_per_folder:\n",
    "            image_rgb = skimage.io.imread(os.path.join(video_path, category,image))\n",
    "            image_nor = normalize(image_rgb)\n",
    "            feature = cnn_feature_extractor(image_nor.view(1,3,224,224).cuda()).cpu().view(1024*7*7)\n",
    "            category_frames.append(feature)\n",
    "        train_all_video_frame.append(torch.stack(category_frames))\n",
    "\n",
    "    print(\"\\nvalidation set .....\")\n",
    "    video_path = \"../HW5_data/FullLengthVideos/videos/valid/\"\n",
    "    category_list = sorted(listdir(video_path))\n",
    "\n",
    "    category = category_list[1]\n",
    "    test_all_video_frame = []\n",
    "    for category in category_list:\n",
    "        print(\"category:\",category)\n",
    "        image_list_per_folder = sorted(listdir(os.path.join(video_path,category)))\n",
    "        category_frames = []\n",
    "        for image in image_list_per_folder:\n",
    "            image_rgb = skimage.io.imread(os.path.join(video_path, category,image))\n",
    "            image_nor = normalize(image_rgb)\n",
    "            feature = cnn_feature_extractor(image_nor.view(1,3,224,224).cuda()).cpu().view(1024*7*7)\n",
    "            category_frames.append(feature)\n",
    "        test_all_video_frame.append(torch.stack(category_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_FullLength_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_all_video_frame, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"valid_FullLength_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_all_video_frame, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut to define size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../features/train_FullLength_features.pkl\", \"rb\") as f:\n",
    "    train_all_video_frame = pickle.load(f)\n",
    "with open(\"../features/valid_FullLength_features.pkl\", \"rb\") as f:\n",
    "    valid_all_video_frame = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ground truth\n",
    "label_path = \"../HW5_data/FullLengthVideos/labels/train/\"\n",
    "category_txt_list = sorted(listdir(label_path))\n",
    "train_category_labels = []\n",
    "for txt in category_txt_list:\n",
    "    file_path = os.path.join(label_path,txt)\n",
    "    with open(file_path,\"r\") as f:\n",
    "        label = [int(w.strip()) for w in f.readlines()]\n",
    "        train_category_labels.append(label)\n",
    "        \n",
    "label_path = \"../HW5_data/FullLengthVideos/labels/valid/\"\n",
    "category_txt_list = sorted(listdir(label_path))\n",
    "valid_category_labels = []\n",
    "for txt in category_txt_list:\n",
    "    file_path = os.path.join(label_path,txt)\n",
    "    with open(file_path,\"r\") as f:\n",
    "        label = [int(w.strip()) for w in f.readlines()]\n",
    "        valid_category_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using \"slice\" function in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_frames(features_per_category, labels_per_category, size = 200, overlap = 20):\n",
    "    feature_size = 50176\n",
    "    a = torch.split(features_per_category, size-overlap)\n",
    "    b = torch.split(torch.Tensor(labels_per_category), size-overlap)\n",
    "\n",
    "    cut_features = []\n",
    "    cut_labels = []\n",
    "    for i in range(len(a)):\n",
    "        if i==0:\n",
    "            cut_features.append(a[i])\n",
    "            cut_labels.append(b[i])\n",
    "        else:\n",
    "            cut_features.append(torch.cat((a[i-1][-overlap:],a[i])))\n",
    "            cut_labels.append(torch.cat((b[i-1][-overlap:],b[i])))\n",
    "    \n",
    "    lengths = [len(f) for f in cut_labels]\n",
    "#     perm_index = np.argsort(lengths)[::-1]\n",
    "#     cut_features =  [cut_features[i] for i in perm_index]\n",
    "#     cut_labels =  [cut_labels[i] for i in perm_index]\n",
    "#     lengths = sorted(lengths, reverse=True)\n",
    "#     cut_features_pad = nn.utils.rnn.pad_sequence(cut_features, batch_first=True)\n",
    "    return cut_features, cut_labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1, r2, r3 = cut_frames(train_all_video_frame[0],train_category_labels[0], size = 120, overlap = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n"
     ]
    }
   ],
   "source": [
    "cutting_steps = 350\n",
    "overlap_steps = 30\n",
    "train_cut_features = []\n",
    "train_cut_labels = []\n",
    "train_cut_lengths = []\n",
    "for category_frames, category_labels in zip(train_all_video_frame,train_category_labels):\n",
    "    features, labels, lengths = cut_frames(category_frames,category_labels, \n",
    "                                           size = cutting_steps, overlap = overlap_steps)\n",
    "    train_cut_features += features\n",
    "    train_cut_labels += labels\n",
    "    train_cut_lengths += lengths\n",
    "    print(\"one category done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_lengths = [len(s) for s in valid_all_video_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2140, 938, 857, 809, 1360]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n",
      "one category done\n"
     ]
    }
   ],
   "source": [
    "# valid ‰∏ç cut\n",
    "\n",
    "# valid_cut_features = []\n",
    "# valid_cut_labels = []\n",
    "# valid_cut_lengths = []\n",
    "# for category_frames, category_labels in zip(valid_all_video_frame,valid_category_labels):\n",
    "#     features, labels, lengths = cut_frames(category_frames,category_labels, \n",
    "#                                            size = cutting_steps, overlap = 0)\n",
    "#     valid_cut_features += features\n",
    "#     valid_cut_labels += labels\n",
    "#     valid_cut_lengths += lengths\n",
    "#     print(\"one category done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../features/train_cut_features_350.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_cut_features,f)\n",
    "with open(\"../features/train_cut_labels_350.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_cut_labels,f)\n",
    "with open(\"../features/train_cut_lengths_350.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_cut_lengths,f)\n",
    "    \n",
    "with open(\"../features/valid_cut_features_no_cut.pkl\", \"wb\") as f:\n",
    "    pickle.dump(valid_all_video_frame,f)\n",
    "with open(\"../features/valid_cut_labels_no_cut.pkl\", \"wb\") as f:\n",
    "    pickle.dump(valid_category_labels,f)\n",
    "with open(\"../features/valid_cut_lengths_no_cut.pkl\", \"wb\") as f:\n",
    "    pickle.dump(valid_lengths,f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
